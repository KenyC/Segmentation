import numpy as np
import scipy.stats as st
from math import log,exp

class HMM:
	# a: k list of letter value: maps index i to letter with index i
	# aR: dictionary (size k) mapping letters to indices.
	# k: size of the alphabet
	# t: transition matrix (n*n)->0,1 
	# i: initial state distribution n->0,1 (probability that i is initial state)
	# e: end state distribution n->0,1 (probability that i is output state)
	# o: output distribution (n*k)->0,1 (probability that state i outputs letter j)
	# n: number of states
	def __init__(self,alphabet,init,end,output,trans):
		self.a=alphabet
		self.k=len(self.a)
		self.aR=dict()
		for i in range(self.k):
			self.aR[self.a[i]]=i
			
		self.t=trans
		self.i=init
		self.e=end
		self.o=output
		
		self.n=len(self.i)
		
		# Auxiliary variable used for picking transition
		self.arr=[i for i in range(self.n)]
		self.arr.append(-1)
	def gen(self,n=-1,path=False):
		if n<0:
			return self.gen_aux(path)
		else:
			l=[]
			for i in range(n):
				l.append(self.gen_aux(path))
			return l
	
	def gen_aux(self,a_path=False):
		path=[]
		word=""
		# Pick initial state randomly
		stateCur=np.random.choice(self.n,p=self.i)
		output=stateCur
		
		# While the state does not trigger output
		while output!=-1:
			stateCur=output
			path.append(stateCur)
			
			# Creating outcomes for transition drawing (either another state or end here)
			drawProba=self.t[stateCur]
			drawProba=np.append(drawProba,self.e[stateCur])
			
			# Drawing transition at random
			output=np.random.choice(self.arr,p=drawProba)
			# Drawing letter at random
			letter=np.random.choice(self.a,p=self.o[stateCur])
			
			# Append new letter
			word+=letter
		
		if a_path:
			return (path,word)
		else:
			return word
	
	# Returns one of the most likely paths that produce the word.
	def viterbi(self,word,probOnly=False,log=False):
		probaL,pathL=self.viterbi_aux(list(word))
		iMax=np.argmax(probaL*self.e)
		probaFin=probaL[iMax]*self.e[iMax]
		
		if not log:
			if probOnly:
				return probaFin
			else:
				return probaFin,pathL[iMax]
		else:
			if probOnly:
				return np.log(probaFin)
			else:
				return np.log(probaFin),pathL[iMax]
		
	# Returns for all i, one of the most likely paths that produce the sequence of characters "seq" that ends in state i.
	def viterbi_aux(self,seq):
		l=len(seq)
		path=[[[] for i in range(self.n)] for j in range(l)]
		proba=np.full((l,self.n),1.0,dtype='float')
		
		for i in range(self.n):
			path[0][i].append(i)
			proba[0][i]=self.i[i]*self.o[i][self.aR[seq[0]]]
		
		for j in range(1,l):
			for i in range(self.n):
				max=0.0
				pmax=[]
				for k in range(self.n):
					p=proba[j-1][k]*self.t[k][i]*self.o[i][self.aR[seq[j]]]
					if p>=max:
						max=p
						pmax=path[j-1][k]+[i]
				proba[j][i]=max
				path[j][i]=pmax
		return (proba[l-1],path[l-1])
	
	# Returns the log-likelihood that the given corpus "obs" is generated
	def logL(self, obs):
		l=0.0
		for o in obs:
			a=self.proba(o)
			if a==0:
				return -np.inf
			else:
				l+=np.log(a)
		return l
		
	# Returns the log-likelihood that the given corpus "obs" was generated with a penalty for order of Markov chain
	def bicScore(self,obs):
		return 2*self.logL(obs)-self.dims()*np.log(len(obs))
	
	# Returns the probability that word is generated by the HMM
	def proba(self,word,logR=False):
		p=self.proba_aux(list(word))
		s=np.sum(p*self.e)
		if logR:
			return np.log(s)
		else:
			return s

	# Returns the array of probability that a given word is generated and and the chain ends at i
	# Dynamic programming: compute the probability that every prefix of the word is generated and ends at state i
	def proba_aux(self,seq):
		l=len(seq)
		proba=np.full((l,self.n),1.0,dtype='float')
		o1=np.swapaxes(self.o,0,1)
		t1=np.swapaxes(self.t,0,1)
		
		proba[0]=self.i*o1[self.aR[seq[0]]]
		
		for t in range(1,l):
			proba[t]=(t1.dot(proba[t-1]))*o1[self.aR[seq[t]]]
				
		return proba[l-1]
	
	# Compute the result of the Baum-Welch algorithm until a desired level of convergence is reached ; the algorithm will try to achieve the desired level of convergence in a prespecified number of steps
	# obs: corpus, observed data
	# convLevel: algorithm stops when difference in log-likelihood obatained after performing one step is below convLevel
	# maxSteps: maximum number of steps to reach desried level of convergence
	def baumwelch(self,obs,convLevel=0.001,maxSteps=10000):
		# difference in log-likelihood for current step ; initial value: anything that is above the threshold
		dLL=convLevel+1.0
		# loop index
		i=0
		
		while i<maxSteps and dLL>=convLevel:
			dLL=self.baumwelch_aux(obs)
			if i%50==0:
				print("step nÂ°%i complete (n states=%i): DLL=%f"%(i,self.n,dLL))
			i+=1
		
		return i
	
	# Compute one step of Baum-Welch algorithm to improve the likelihood of generating a given corpus ; returns the difference in log-likelihood of generating corpus before and after the algorithm has applied
	# obs: corpus, observed data
	def baumwelch_aux(self,obs):
		
		# Expected number of transition out of state i
		expSt=np.full(self.n,0.0,dtype='float')
		# Expected number of transition from i to j
		expTr=np.full((self.n,self.n),0.0,dtype='float')
		# Expected number of starts in i
		expInit=np.full(self.n,0.0,dtype='float')
		# Expected number of endings in i
		expEnd=np.full(self.n,0.0,dtype='float')
		# Expected number of outputting letter a in i
		expOutput=np.full((self.k,self.n),0.0,dtype='float')
		# Difference in log-likelihood of generating corpus before and after algorithm
		dLL=0.0
		
		# Transition variable
		t1=np.swapaxes(self.t,0,1)
		o1=np.swapaxes(self.o,0,1)
		
		for sComp in obs:
			s=list(sComp)
			nC=[self.aR[c] for c in s]
			l=len(s)
			# print(nC)
			
			
			
			# Forward procedure: compute the probability of observing the prefix up until t and end in state i
			forward=np.full((l,self.n),0.0,dtype='float')
			forward[0]=self.i*o1[nC[0]]
			for t in range(l-1):
				forward[t+1]=(t1.dot(forward[t]))*o1[nC[t+1]]
			# print("Forward:",forward)
			
			# Backward procedure: compute the probability of ending with suffix from t on and start with state i
			backward=np.full((l,self.n),1.0,dtype='float')
			backward[l-1]=self.e
			for t in range(l-1,0,-1):
				backward[t-1]=self.t.dot(backward[t]*o1[nC[t]])
			# print("Backward:",backward)
			
			# Computing probability of outputting s
			proba=np.sum(forward[l-1]*backward[l-1])
			# Updating log likelihood
			dLL-=np.log(proba)
			
			# Computing probability of being in i at time t given the string s
			# This is equal to the probability of outputting the string s while stopping at i divided by the probability of outputting s (P(A/B)=P(A and B)/P(B))
			gamma=forward*backward
			gamma=(1/proba)*gamma
			# print("Gamma",gamma)
			
			# Computing probability of transitioning from i to j at time t given the string s
			# This is equal to the probability of outputting the string s while transitioning from i to j divded by the probability of outputting s (P(A/B)=P(A and B)/P(B))
			zeta=np.full((l-1,self.n,self.n),1.0,dtype='float')
			f=forward[0:l-1]
			b=backward[1:l]
			for t in range(l-1):
				aux=f[t]*np.transpose(zeta[t])
				zeta[t]=self.t*o1[nC[t+1]]*b[t]*np.transpose(aux)
			zeta=(1/proba)*zeta
			
			# Update estimates
			expEnd+=gamma[l-1]
			expInit+=gamma[0]
			expTr+=np.sum(zeta,axis=0)
			for t in range(l):
				expOutput[nC[t]]+=gamma[t]
		# Updating values
		self.i=expInit/np.sum(expInit)
		self.e=expEnd
		self.t=expTr
		self.o=np.transpose(expOutput)

		
		# Normalizing the result
		normTr=self.e+np.sum(self.t,axis=1)
		normOut=np.sum(self.o,axis=1)
		self.e/=normTr
		for i in range(self.n):
			self.t[i]/=normTr[i]
			self.o[i]/=normOut[i]
		
		# Compute new likelihood and use it to compute the difference in likelihood
		dLL+=self.logL(obs)
		return dLL
	
	# Display important parameter of the HMM, optionally (depending on "round") rounding them to "nD" decimals after the decimal point
	def aff(self,round=True,nD=3):
		if round:
			print("Init:",np.round(self.i,nD))
			print("End:",np.round(self.e,nD))
			print("Output:",np.round(self.o,nD))
			print("Transition:",np.round(self.t,nD))
		else:
			print("Init:",self.i)
			print("End:",self.e)
			print("Output:",self.o)
			print("Transition:",self.t)
	# Returns the number of degrees of freedom in the model
	def dims(self):
		return self.n*(self.n+self.k)-1
	
	def randHMM(alphabet,n):
		k=len(alphabet)
		init=np.random.sample(n)
		init/=np.sum(init)
		output=np.random.sample((n,k))
		output/=np.sum(output)
		
		inter=np.random.sample((n,n+1))
		trans=np.full((n,n),0.0,dtype='float')
		end=np.full(n,0.0,dtype='float')
		for i in range(n):
			output[i]/=np.sum(output[i])
			inter[i]/=np.sum(inter[i])
			trans[i]=inter[i][:-1]
			end[i]=inter[i][-1]
		
		return HMM(alphabet,init,end,output,trans)
	
	 
	# Returns a flattened array that contains all the parameters of the HMM
	def flat(self):
		flat=self.i
		flat=np.append(flat,self.o.flatten())
		flat=np.append(flat,self.e)
		flat=np.append(flat,self.t.flatten())
		return flat
		
	# Returns a sequence of the parameter vectors of the HMM.
	# The parameter vectors are vectors whose coordinates sum up to 1 that together completely specify the model.
	# The parameter vectors are returned in the following arbitrary (but convenient) way:
	# Transition/End+Output+Init
	def parameterVector(self):
		pV=list(self.e.copy())+list(self.o.copy())+[self.i.copy()]
		for i in range(self.n):
			pV[i]=np.append(pV[i],self.t.copy()[i])
		return pV
		
	def fromParameterVector(alphabet,pV,order):
		toSplit=pV[:order]
		output=np.array(pV[order:2*order])
		init=pV[-1]
		trans=np.full((order,order),0.0)
		end=np.full((order),0.0)
		
		for i in range(order):
			trans[i]=toSplit[i][1:]
			end[i]=toSplit[i][0]
		
		return HMM(alphabet,init,end,output,trans)
		
	
	# Returns the Euclidian distance between all the parameters of "hm1" and "hm2"
	def dist(hm1,hm2):
		return np.linalg.norm(hm1.flat()-hm2.flat())
	
	
			
			