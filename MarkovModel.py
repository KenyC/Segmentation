import numpy as np
# import scipy.stats as st
# from math import log,exp

class HMM:
	# a: k list of letter value: maps index i to letter with index i
	# aR: dictionary (size k) mapping letters to indices.
	# k: size of the alphabet

	# t: transition matrix (n*k*n)->0,1 
	# i: initial state distribution n->0,1 (probability that i is initial state)
	# e: end state distribution n->0,1 (probability that i is output state)
	# n: number of states

	def __init__(self,alphabet,init,end,trans):
		self.a=alphabet
		self.k=len(self.a)
		self.aR=dict()
		for i in range(self.k):
			self.aR[self.a[i]]=i
			
		self.t=trans
		self.i=init
		self.e=end
		
		self.n=len(self.i)
		
		# Auxiliary variable used for picking transition
		self.pickTransition = [(state,letter) for letter in range(self.k) for state in range(self.n)]
		self.pickTransition.append((-1,0))


	def gen(self,n=-1,path=False):
		if n<0:
			return self.gen_aux(path)
		else:
			l=[]
			for i in range(n):
				l.append(self.gen_aux(path))
			return l
	
	def gen_aux(self,a_path=False):
		path=[]
		word=""
		# Pick initial state randomly
		stateCur=np.random.choice(self.n,p=self.i)
		output=stateCur
		
		# While the state does not trigger output
		while output!=-1:
			stateCur=output
			path.append(stateCur)
			
			# Creating outcomes for transition drawing (either another state or end here)
			drawProba=self.t[stateCur].flatten()
			drawProba=np.append(drawProba,self.e[stateCur])
			
			# Drawing transition at random
			idxChoice = np.random.choice(len(self.pickTransition),p=drawProba)
			output,letter = self.pickTransition[idxChoice]
			letter = self.a[letter]
			
			# Append new letter
			if output!=-1:
				word+=letter
		
		if a_path:
			return (path,word)
		else:
			return word
	
	# Returns one of the most likely paths that produce the word.
	# - word: the word
	# - probOnly: if True, does not return path
	# - log: if True, returns log-proba
	def viterbi(self,word,probOnly=False,log=False):
		probaL,pathL=self.viterbi_aux(list(word))
		iMax=np.argmax(probaL*self.e)
		probaFin=probaL[iMax]*self.e[iMax]
		
		if not log:
			if probOnly:
				return probaFin
			else:
				return probaFin,pathL[iMax]
		else:
			if probOnly:
				return np.log(probaFin)
			else:
				return np.log(probaFin),pathL[iMax]
		
	# Returns for all i, one of the most likely paths that produce the sequence of characters "seq" that ends in state i.
	# - seq: a list of characters that compose the word
	def viterbi_aux(self,seq):
		l = len(seq)

		path = [[[] for i in range(self.n)] for j in range(l+1)]
		proba=np.full((l+1,self.n),1.0,dtype='float')
		
		for i in range(self.n):
			path[0][i].append(i)
			proba[0][i]=self.i[i]#*self.o[i][self.aR[seq[0]]]
		
		for j in range(1,l+1):
			for i in range(self.n):

				maxProba=0.0
				pmax=[]
				
				for k in range(self.n):
					
					p=proba[j-1][k]*self.t[k][self.aR[seq[j-1]]][i]
					
					if p>=maxProba:
						maxProba=p
						pmax=path[j-1][k]+[i]
				
				proba[j][i]=maxProba
				path[j][i]=pmax
		
		return (proba[l],path[l])
	
	# Returns the log-likelihood that the given corpus "obs" is generated
	def logL(self, obs):
		l=0.0
		for o in obs:
			a=self.proba(o)
			if a==0:
				return -np.inf
			else:
				l+=np.log(a)
		return l
		
	# Returns the log-likelihood that the given corpus "obs" was generated with a penalty for order of Markov chain
	def bicScore(self,obs):
		return 2*self.logL(obs)-self.dims()*np.log(len(obs))
	
	# Returns the probability that word is generated by the HMM
	def proba(self,word,logR=False):
		p=self.proba_aux(list(word))
		s=np.sum(p*self.e)
		if logR:
			return np.log(s)
		else:
			return s

	# Returns the array of probability that a given word is generated and and the chain ends at i
	# Dynamic programming: compute the probability that every prefix of the word is generated and ends at state i
	# - seq: the sequence of characters that constitute the word
	def proba_aux(self,seq):
		l=len(seq)
		proba=np.full((l+1,self.n),1.0,dtype='float')
		# Put the array in the form (letter, output_state, input_state)
		t1=np.transpose(self.t,(1,2,0))
		
		proba[0]=self.i
		
		for t in range(1,l+1):
			letter = seq[t-1]
			proba[t]=(t1[self.aR[letter]].dot(proba[t-1]))
				
		return proba[l]


	# Compute the result of the Baum-Welch algorithm until a desired level of convergence is reached ; the algorithm will try to achieve the desired level of convergence in a prespecified number of steps
	# obs: corpus, observed data
	# convLevel: algorithm stops when difference in log-likelihood obatained after performing one step is below convLevel
	# maxSteps: maximum number of steps to reach desried level of convergence
	def baumwelch(self,obs,convLevel=0.001,maxSteps=10000,display = 50):
		# difference in log-likelihood for current step ; initial value: anything that is above the threshold
		dLL=convLevel+1.0
		# loop index
		i=0
		
		while i<maxSteps and dLL>=convLevel:
			dLL=self.baumwelch_aux(obs)
			if i%display==0:
				print("step nÂ°%i complete (n states=%i): DLL=%f"%(i,self.n,dLL))
			i+=1
		
		return i
	
	# Compute one step of Baum-Welch algorithm to improve the likelihood of generating a given corpus ; returns the difference in log-likelihood of generating corpus before and after the algorithm has applied
	# obs: corpus, observed data
	def baumwelch_aux(self,obs):
		
		# Expected number of transition from i to j with letter l
		expTr=np.full((self.k, self.n, self.n),0.0,dtype='float')
		# Expected number of starts in i
		expInit=np.full(self.n,0.0,dtype='float')
		# Expected number of endings in i
		expEnd=np.full(self.n,0.0,dtype='float')
		# Difference in log-likelihood of generating corpus before and after algorithm
		dLL=0.0
		
		# Transition variable
		t1=np.transpose(self.t,(1,2,0))
		t2=np.transpose(self.t,(1,0,2))
		
		for sComp in obs:
			#Convert input sequence into list of indices
			s=list(sComp)
			nC=[self.aR[c] for c in s]

			l=len(s)
			# print(nC)
			
			
			
			# Forward procedure: compute the probability of observing the prefix up until t and end in state i
			forward = np.full((l+1,self.n),0.0,dtype='float')
			forward[0] = self.i
			for t in range(l):
				forward[t+1]=t1[nC[t]].dot(forward[t])
			# print("Forward:",forward)
			
			# Backward procedure: compute the probability of ending with suffix from t on and start with state i
			backward=np.full((l+1,self.n),1.0,dtype='float')
			backward[l]=self.e
			for t in range(l,0,-1):
				backward[t-1]=t2[nC[t-1]].dot(backward[t])
			# print("Backward:",backward)
			
			# Computing probability of outputting s
			proba=np.sum(forward[l]*backward[l])
			# Updating log likelihood
			dLL-=np.log(proba)
			
			# Computing probability of being in i at time t given the string s
			# This is equal to the probability of outputting the string s while stopping at i divided by the probability of outputting s (P(A/B)=P(A and B)/P(B))
			gamma=forward*backward
			gamma=(1/proba)*gamma
			# print("Gamma",gamma)
			
			# Computing probability of transitioning from i to j at time t given the string s
			# This is equal to the probability of outputting the string s while transitioning from i to j divded by the probability of outputting s (P(A/B)=P(A and B)/P(B))
			zeta=np.full((l,self.n,self.n),1.0,dtype='float')
			
			f = np.tile(forward[0:l],(self.n,1,1))
			f = np.transpose(f,(1,2,0))

			b = np.tile(backward[1:l+1],(self.n,1,1))
			b = np.transpose(b,(1,0,2))

			for t in range(l):
				zeta[t]=t2[nC[t]]*b[t]*f[t]
			
			zeta=(1/proba)*zeta
			
			# Update estimates
			expEnd+=gamma[l]
			expInit+=gamma[0]
			for t in range(l):
				expTr[nC[t]]+=zeta[t]

		# Updating values
		self.i=expInit/np.sum(expInit)
		self.e=expEnd
		self.t=np.transpose(expTr,(1,0,2))
		

		
		# Normalizing the result
		normTr=self.e+np.sum(self.t,axis=(1,2))
		self.e/=normTr
		for i in range(self.n):
			self.t[i]/=normTr[i]
		
		# Compute new likelihood and use it to compute the difference in likelihood
		dLL+=self.logL(obs)
		return dLL
	
	# Display important parameter of the HMM, optionally (depending on "round") rounding them to "nD" decimals after the decimal point
	def aff(self,nD=3):

		print("Init:")
		for i in range(self.n):
			print("\tState {} : {}".format(i,np.round(self.i[i],nD)))

		print()
		print("Transition/End:")
		for inS in range(self.n):
			print()
			print("\t{} -- -->#:  {}".format(inS,np.round(self.e[inS],nD)))
			for outS in range(self.n):
				for letter in range(self.k):
					print("\t{} --{}-->{}:  {}".format(inS,self.a[letter],outS,np.round(self.t[inS][letter][outS],nD)))



	# Returns the number of degrees of freedom in the model
	def dims(self):
		return self.n*(self.n+self.k)-1
	
	def randHMM(alphabet,n):
		k=len(alphabet)

		init=np.random.sample(n)
		init/=np.sum(init)
		
		inter=np.random.sample((n,n*k+1))
		trans=np.full((n,k,n),0.0,dtype='float')
		end=np.full(n,0.0,dtype='float')
		
		for i in range(n):
			inter[i]/=np.sum(inter[i])
			trans[i]=inter[i][:-1].reshape(k,n)
			end[i]=inter[i][-1]
		
		return HMM(alphabet,init,end,trans)
	
	 
	# Returns a flattened array that contains all the parameters of the HMM
	def flat(self):
		flat=self.i
		flat=np.append(flat,self.o.flatten())
		flat=np.append(flat,self.e)
		flat=np.append(flat,self.t.flatten())
		return flat
	
	# Normalizes the parameters of the HMM
	def normalize(self):
		self.i/=np.sum(self.i)

		print(self.t.shape)
		print(self.e.shape)
		for i in range(self.n):
			norm = np.sum(self.t[i])+self.e[i]
			self.e[i] /= norm
			self.t[i] /= norm

	# Returns a sequence of the parameter vectors of the HMM.
	# The parameter vectors are vectors whose coordinates sum up to 1 that together completely specify the model.
	# The parameter vectors are returned in the following arbitrary (but convenient) way:
	# Transition/End+Output+Init
	def parameterVector(self):
		pV=list(self.e.copy())+list(self.o.copy())+[self.i.copy()]
		for i in range(self.n):
			pV[i]=np.append(pV[i],self.t.copy()[i])
		return pV
		
	def fromParameterVector(alphabet,pV,order):
		toSplit=pV[:order]
		output=np.array(pV[order:2*order])
		init=pV[-1]
		trans=np.full((order,order),0.0)
		end=np.full((order),0.0)
		
		for i in range(order):
			trans[i]=toSplit[i][1:]
			end[i]=toSplit[i][0]
		
		return HMM(alphabet,init,end,output,trans)
		
	
	# Returns the Euclidian distance between all the parameters of "hm1" and "hm2"
	def dist(hm1,hm2):
		return np.linalg.norm(hm1.flat()-hm2.flat())

	def jitter(self,epsilon = 1000, clamp=0.001):
		for i in range(self.n):
			flatTr = np.append(self.t[i].flatten(),self.e[i])
			for j in range(self.n*self.k+1):
				if flatTr[j]<clamp:
					flatTr[j] = clamp
				elif flatTr[j]>1-clamp:
					flatTr[j] = 1-clamp
			flatTr*=epsilon
			newValues = np.random.dirichlet(flatTr)
			self.e[i]=newValues[-1]
			self.t[i]=newValues[:-1].reshape(self.k,self.n)
			# for chaining
			return self

	
##############FOR TESTING##################################
			
alphabet = ['a','b']
sizeAlph = len(alphabet)

# HMM 1
# - generates alternations of large 'a' sequences and smaller 'a' sequences separated by single b's
def HMM1():
	
	nStates = 2

	init = np.full(nStates,0.0)
	init[0]=1.0

	trans = np.full((nStates,sizeAlph,nStates),0.0)
	end = np.full(nStates,0.0)

	# From initial state
	trans[0][0][0]=0.9
	trans[0][1][1]=0.05
	end[0]=0.05

	# From other state
	trans[1][0][1]=0.6
	trans[1][1][0]=0.35
	end[1]=0.05

	return HMM(alphabet,init,end,trans)

# HMM 2
# - just like the above with the added complication that initial state is random
# - and the first state may sometimes output b's and stay in the same state
def HMM2():
	
	nStates = 2

	init = np.full(nStates,0.5)
	

	trans = np.full((nStates,sizeAlph,nStates),0.0)
	end = np.full(nStates,0.0)

	# From initial state
	trans[0][0][0]=0.8
	trans[0][1][0]=0.1
	trans[0][1][1]=0.05
	end[0]=0.05

	# From other state
	trans[1][0][1]=0.6
	trans[1][1][0]=0.35
	end[1]=0.05

	return HMM(alphabet,init,end,trans)

# HMM 3
# - produces a bunch of a followed by a bunch of b
def HMM3():
	
	nStates = 2

	init = np.full(nStates,0.1)
	init[0]=0.9
	

	trans = np.full((nStates,sizeAlph,nStates),0.0)
	end = np.full(nStates,0.0)

	# From initial state
	trans[0][0][0]=0.9
	trans[0][1][1]=0.05
	end[0]=0.05

	# From other state
	trans[1][1][1]=0.7
	end[1]=0.3

	return HMM(alphabet,init,end,trans)

def getAlph(corpus):
	return list(set().union(*tuple(set(w) for w in corpus)))
#
# def compBaumWelch(corpus,n,tParams={'convLevel':0.001,
# 	'maxSteps': 10000,
# 	'epsilonJitter': 1000,
# 	'clampJitter': 0.001,
# 	'pLogistic': 3.0,
# 	'overflow': 10.0,
# 	'rateOverflow': 0.1}):
# 	alphabet = getAlph(corpus)

# 	# Two competitor HMMs
# 	H1 = randHMM(corpus,n)
# 	H2 = randHMM(corpus,n)

# 	H1.name = "H1"
# 	H2.name = "H2"

	

# 	# the first competitor is the one that is being trained
# 	current = H1,H2

# 	# Initial log-likelihoods
# 	ll = H1.logL(corpus), H2.logL(corpus)

# 	# difference in log-likelihood for current step ; initial value: anything that is above the threshold
# 	dLL=convLevel+1.0

# 	# loop index
# 	i=0
	
# 	overP = tParams['overflow']
# 	while i<maxSteps and dLL>=convLevel:

# 		dLL=current[0].baumwelch_aux(obs)
# 		ll[0] = current[0].logL(corpus)
# 		ll[1] = current[1].jitter(epsilon = tParams['epsilonJitter'],clamp = tParams['clampJitter']).logL(corpus)

# 		overP = (1-tParams['overflowRate']) * overP + tParams['overflowRate']

# 		probaSwitch = logistic.cdf((ll[1]-ll[0])/tParams['pLogistic'])
# 		probaSwitch *= overP
# 		probaSwitch = 1 if probaSwitch > 1 else probaSwitch

# 		if np.random.binomial(1,probaSwitch):
# 			current = current[1],current[0]
# 			ll = ll[1], ll[0]
# 			overP = tParams['overflow']

# 		if i%50==0:
# 			print("step nÂ°{} complete (n states={}): DLL={}, current={}".format(i,self.n,dLL,current[0].name))
# 		i+=1
	
	

# 	#return {'bestHMM': ,'num_steps': i, 'finalLL': }




# def indBaumWelch(corpus,n,convLevel=0.001,maxSteps=10000):
# 	alphabet = getAlph(corpus)
# 	H1 = randHMM(corpus,n)
# 	H2 = randHMM(corpus,n)

# 	# difference in log-likelihood for current step ; initial value: anything that is above the threshold
# 	dLL=convLevel+1.0
# 	# loop index
# 	i=0
	
# 	while i<maxSteps and dLL>=convLevel:
# 		dLL=H1.baumwelch_aux(obs)
# 		if i%50==0:
# 			print("step nÂ°%i complete (n states=%i): DLL=%f"%(i,n,dLL))
# 		i+=1
	
# 	return i

